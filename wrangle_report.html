<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=AQiK5OY2LyZbJ5C4dTWLWPCTxt476E0BHRHQ-Eg4NTM');.lst-kix_yvs0ndjr0wx9-4>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-4}ol.lst-kix_yvs0ndjr0wx9-1.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-1 0}ol.lst-kix_yvs0ndjr0wx9-4.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-4 0}.lst-kix_yvs0ndjr0wx9-5>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-5}ol.lst-kix_yvs0ndjr0wx9-0{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-1{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-2{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-3{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-4{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-5{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-6{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-7{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-8{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-2.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-2 0}ol.lst-kix_yvs0ndjr0wx9-8.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-8 0}ol.lst-kix_yvs0ndjr0wx9-5.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-5 0}ul.lst-kix_q4nz7u4uxz21-0{list-style-type:none}.lst-kix_q4nz7u4uxz21-1>li:before{content:"\0025cb  "}ul.lst-kix_q4nz7u4uxz21-1{list-style-type:none}.lst-kix_yvs0ndjr0wx9-1>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-1}.lst-kix_q4nz7u4uxz21-0>li:before{content:"\0025cf  "}.lst-kix_q4nz7u4uxz21-2>li:before{content:"\0025a0  "}.lst-kix_q4nz7u4uxz21-3>li:before{content:"\0025cf  "}.lst-kix_yvs0ndjr0wx9-7>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-7}ol.lst-kix_yvs0ndjr0wx9-6.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-6 0}.lst-kix_yvs0ndjr0wx9-7>li:before{content:"" counter(lst-ctn-kix_yvs0ndjr0wx9-7,lower-latin) ". "}.lst-kix_q4nz7u4uxz21-5>li:before{content:"\0025a0  "}.lst-kix_q4nz7u4uxz21-4>li:before{content:"\0025cb  "}.lst-kix_q4nz7u4uxz21-6>li:before{content:"\0025cf  "}.lst-kix_yvs0ndjr0wx9-8>li:before{content:"" counter(lst-ctn-kix_yvs0ndjr0wx9-8,lower-roman) ". "}.lst-kix_yvs0ndjr0wx9-2>li:before{content:"" counter(lst-ctn-kix_yvs0ndjr0wx9-2,lower-roman) ") "}.lst-kix_q4nz7u4uxz21-8>li:before{content:"\0025a0  "}.lst-kix_q4nz7u4uxz21-7>li:before{content:"\0025cb  "}.lst-kix_yvs0ndjr0wx9-3>li:before{content:"(" counter(lst-ctn-kix_yvs0ndjr0wx9-3,decimal) ") "}ul.lst-kix_q4nz7u4uxz21-8{list-style-type:none}.lst-kix_yvs0ndjr0wx9-5>li:before{content:"(" counter(lst-ctn-kix_yvs0ndjr0wx9-5,lower-roman) ") "}.lst-kix_yvs0ndjr0wx9-6>li:before{content:"" counter(lst-ctn-kix_yvs0ndjr0wx9-6,decimal) ". "}ul.lst-kix_q4nz7u4uxz21-6{list-style-type:none}.lst-kix_yvs0ndjr0wx9-2>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-2}.lst-kix_yvs0ndjr0wx9-8>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-8}ul.lst-kix_q4nz7u4uxz21-7{list-style-type:none}ul.lst-kix_q4nz7u4uxz21-4{list-style-type:none}ul.lst-kix_q4nz7u4uxz21-5{list-style-type:none}.lst-kix_yvs0ndjr0wx9-4>li:before{content:"(" counter(lst-ctn-kix_yvs0ndjr0wx9-4,lower-latin) ") "}ul.lst-kix_q4nz7u4uxz21-2{list-style-type:none}ul.lst-kix_q4nz7u4uxz21-3{list-style-type:none}ol.lst-kix_yvs0ndjr0wx9-7.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-7 0}.lst-kix_yvs0ndjr0wx9-0>li:before{content:"" counter(lst-ctn-kix_yvs0ndjr0wx9-0,decimal) ") "}.lst-kix_yvs0ndjr0wx9-1>li:before{content:"" counter(lst-ctn-kix_yvs0ndjr0wx9-1,lower-latin) ") "}.lst-kix_yvs0ndjr0wx9-6>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-6}ol.lst-kix_yvs0ndjr0wx9-3.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-3 0}.lst-kix_yvs0ndjr0wx9-3>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-3}ol.lst-kix_yvs0ndjr0wx9-0.start{counter-reset:lst-ctn-kix_yvs0ndjr0wx9-0 0}.lst-kix_yvs0ndjr0wx9-0>li{counter-increment:lst-ctn-kix_yvs0ndjr0wx9-0}ol{margin:0;padding:0}table td,table th{padding:0}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Lora";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Georgia";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Lora";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c13{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c19{color:#000000;text-decoration:none;vertical-align:baseline;font-size:12pt;font-style:normal}.c18{color:#000000;text-decoration:none;vertical-align:baseline;font-size:13pt;font-style:normal}.c14{color:#434343;text-decoration:none;vertical-align:baseline;font-size:12pt;font-style:normal}.c5{color:#000000;text-decoration:none;vertical-align:baseline;font-size:14pt;font-style:normal}.c16{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt}.c17{background-color:#ffffff;max-width:468pt;padding:36pt 72pt 36pt 72pt}.c9{font-weight:700;font-family:"Georgia"}.c20{font-weight:400;font-family:"Roboto"}.c11{margin-left:36pt;padding-left:0pt}.c15{padding:0;margin:0}.c4{font-weight:400;font-family:"Courier New"}.c1{font-weight:400;font-family:"Georgia"}.c7{height:11pt}.c10{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c17"><p class="c12"><span class="c6">Wrangling on a Dataset Report </span></p><p class="c12 c7"><span class="c6"></span></p><p class="c12"><span class="c2">Data Analyst Nanondegree, Project 4: Wrangle and Analyze Data</span></p><p class="c12 c7"><span class="c5 c1"></span></p><p class="c3"><span class="c0">In this report, I will be explaining the steps I took and the decisions I have taken to clean the dataset I was given.</span></p><p class="c3"><span class="c0">I will divide it into three parts, Gathering, Assessing, then Cleaning.</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c1 c18">Tools Used:</span></p><ul class="c15 lst-kix_q4nz7u4uxz21-0 start"><li class="c3 c11"><span class="c0">Python programming language</span></li><li class="c3 c11"><span class="c0">Jupyter Notebook (IPython)</span></li><li class="c3 c11"><span class="c0">Pandas, Requests, and Tweepy Libraries</span></li></ul><p class="c3 c7"><span class="c13 c20"></span></p><p class="c3"><span class="c5 c1">Gathering:</span></p><p class="c3"><span class="c1">At first, I was given a .csv file which I seamlessly loaded with pandas into a dataframe called </span><span class="c1 c10">tw_archive</span><span class="c0">.</span></p><p class="c3"><span class="c1">Then I downloaded the second required .tsv file from the given URL using the requests library, to load it again into another dataframe called </span><span class="c1 c10">tweet_info</span><span class="c0">.</span></p><p class="c3"><span class="c0">Lastly, I used the Twitter API through the Tweepy library to get the missing required info. I got the retweet count and favorite count for each tweet, alongside the image URL of the tweet (Where appropriate)</span></p><p class="c3"><span class="c1">Downloading them took around 15-20 minutes. I stored the data on a new text file (the text itself was formatted in JSON), then I retrieved them into another dataframe called </span><span class="c1 c10 c16">image_pred</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c1 c5">Assessing:</span></p><p class="c3 c7"><span class="c5 c1"></span></p><p class="c12"><span class="c1 c14">At this stage, I&rsquo;m looking for data quality issues, which could be missing data or incorrect data, and tidiness issues, which are poorly designed tables and datasets.</span></p><p class="c3 c7"><span class="c1 c19"></span></p><p class="c3"><span class="c1">I started with visual assessment, by exploring each dataframe individually, looking for any unusual or noticeable quality or tidiness issues. I used functions such as, </span><span class="c4">.head(), .tail(), .sample(n)</span><span class="c1">, and whenever needed I examined each row or couple of rows closely using indexing: </span><span class="c13 c4">.iloc[columns, rows]</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c0">I found some issues quickly such as a redundant &lsquo;+0000&rsquo; string appended to each timestamp, and that the 4 columns that define the dog stage better be collapsed into one column - a tidy data issue. Also, I noted that I should merge the original Twitter archive data with the data I gathered using the Twitter API.</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c0">While other issues weren&rsquo;t easy to notice like values other than 10 for rating denominator, or and that some names were weird: a, an, the, very. </span></p><p class="c3"><span class="c1">Those issues required programmatic assessment to confirm, for which I used functions like </span><span class="c4 c13">.info(), column.value_counts().</span></p><p class="c3"><span class="c1">So for example, I used </span><span class="c4">name.value_counts()</span><span class="c0">&nbsp;to check how many names were invalid.</span></p><p class="c3"><span class="c1">Not to forget, to check for datatype issues, I had to use .info() function. I found that most columns had incorrect, or at least inappropriate, datatypes. Like the </span><span class="c1 c10">id </span><span class="c1">stored as an </span><span class="c9">int </span><span class="c1">instead of a </span><span class="c9">string</span><span class="c0">.</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c0">In the end, I documented all the issues I noted. Using the following hierarchy:</span></p><p class="c3"><span class="c0">Quality Issues:</span></p><p class="c3"><span class="c0">&nbsp;Table &nbsp;1:</span></p><p class="c3"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1)..</span></p><p class="c3"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2)..</span></p><p class="c3"><span class="c0">&nbsp;Table 2:</span></p><p class="c3"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1)..</span></p><p class="c3"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2)..</span></p><p class="c3"><span class="c0">Tidiness Issues:</span></p><p class="c3"><span class="c0">&nbsp;1)</span></p><p class="c3"><span class="c0">&nbsp;2)</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c5 c1">Cleaning:</span></p><p class="c12"><span class="c14 c1">Fixing all the issues I have documented.</span></p><p class="c12 c7"><span class="c14 c1"></span></p><p class="c3"><span class="c0">I fixed some of the datatypes issues at first, converting every id (like tweet_id) to string datatype. This is important for the next step.</span></p><p class="c3"><span class="c0">Next is tidiness issues. I prefer to do them as early as possible so that cleaning other quality issues becomes easier. </span></p><p class="c3"><span class="c0">I merged the tw_archive df with the tweet_info df on the tweet_id, which is why I needed to convert the datatypes as appropriate. Now I had one dataframe that had all the required info of every tweet.</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c0">After tidiness issues, I went to fix the other quality issues. Most of them only required few actions to fix: &nbsp;Converting data types, finding and replacing strings, extracting matches with regex expressions, finding and filling NaN values and some simple math to fix rating issues.</span></p><p class="c3 c7"><span class="c0"></span></p><p class="c3"><span class="c0">Some issues had only one reasonable solution: Dropping the problematic rows. That&rsquo;s because I either couldn&rsquo;t find the missing or correct values or because it was required to remove them.</span></p><p class="c8 c7"><span class="c0"></span></p><p class="c8"><span class="c0">During the cleaning process, I have discovered new issues and went up to document them. </span></p><p class="c8"><span class="c0">Furthermore, as I start to clean an issue, sometimes I change its definition because I would have edited the data and the data frames. And other times I find that the issue has been resolved on itself. To illustrate, I have documented that I should remove a specific invalid tweet. But then when I arrived at it, I found that the tweet has already been removed by a previous drop of data.</span></p></body></html>